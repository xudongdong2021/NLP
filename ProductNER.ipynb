{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MyETdB-dkBsX"
   },
   "source": [
    "## SUNDAN商品命名实体识别（基于BERT微调）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IEnlUbgm8z3B",
    "ExecuteTime": {
     "end_time": "2023-08-27T03:14:11.524544400Z",
     "start_time": "2023-08-27T03:14:11.501486700Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertTokenizer, BertForTokenClassification\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "Sm1krxJtKxpx",
    "outputId": "c0cf11ba-17ff-4a35-da11-5b7b0c4cb11d",
    "tags": [],
    "ExecuteTime": {
     "end_time": "2023-08-27T03:14:12.099295400Z",
     "start_time": "2023-08-27T03:14:12.060472900Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "from torch import cuda\n",
    "device = 'cuda' if cuda.is_available() else 'cpu'\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ahwMsmyG5ZPE"
   },
   "source": [
    "#### **数据处理**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "B676EED6519845328B11B1C9D0A27AD3",
    "ExecuteTime": {
     "end_time": "2023-08-27T03:14:17.193918900Z",
     "start_time": "2023-08-27T03:14:17.157850900Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "                                              sentence  \\\n704  东 芝 多 功 能 彩 色 复 合 机 激 光 双 面 打 印 复 印 扫 描 含 双 纸 ...   \n756  柯 尼 卡 美 能 达 黑 白 多 功 能 复 合 机 主 机 双 面 器 双 面 自 动 ...   \n707    长 城 黑 白 激 光 复 印 机 适 配 国 产 平 台 自 动 双 面 打 印 、 复 印   \n699        联 想 激 光 多 功 能 一 体 机 四 合 一 黑 白 页 双 面 网 络 电 话   \n689        惠 普 喷 墨 一 体 机 幅 面 彩 色 页 分 钟 打 印 扫 描 复 印 传 真   \n705          奔 图 黑 白 激 光 打 印 机 适 配 国 产 平 台 自 动 双 面 打 印   \n706          奔 图 彩 色 激 光 打 印 机 适 配 国 产 平 台 自 动 双 面 打 印   \n754          柯 尼 卡 美 能 达 黑 白 复 印 机 配 双 面 器 双 面 自 动 输 稿   \n682          柯 尼 卡 美 能 达 双 面 自 动 进 稿 器 黑 白 多 功 能 复 合 机   \n694          鸿 合 会 议 平 板 锐 系 列 交 互 电 子 白 板 教 学 一 体 机 英   \n\n                                           word_labels  len_sen  \n704  B-BRA,I-BRA,O,O,O,O,O,B-PRO,I-PRO,I-PRO,O,O,O,...       31  \n756  O,O,O,O,O,O,O,O,B-PRO,I-PRO,I-PRO,I-PRO,I-PRO,...       27  \n707  B-BRA,I-BRA,O,O,O,O,B-PRO,I-PRO,I-PRO,O,O,O,O,...       24  \n699  B-BRA,I-BRA,O,O,B-PRO,I-PRO,I-PRO,I-PRO,I-PRO,...       22  \n689  B-BRA,I-BRA,B-PRO,I-PRO,I-PRO,I-PRO,I-PRO,O,O,...       22  \n705  B-BRA,I-BRA,O,O,O,O,B-PRO,I-PRO,I-PRO,O,O,O,O,...       21  \n706  B-BRA,I-BRA,O,O,O,O,B-PRO,I-PRO,I-PRO,O,O,O,O,...       21  \n754  O,O,O,O,O,O,O,O,B-PRO,I-PRO,I-PRO,O,O,O,O,O,O,...       21  \n682  O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,B-PRO,I-PRO,I-PR...       21  \n694  B-BRA,I-BRA,B-PRO,I-PRO,I-PRO,I-PRO,O,O,O,O,O,...       21  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>sentence</th>\n      <th>word_labels</th>\n      <th>len_sen</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>704</th>\n      <td>东 芝 多 功 能 彩 色 复 合 机 激 光 双 面 打 印 复 印 扫 描 含 双 纸 ...</td>\n      <td>B-BRA,I-BRA,O,O,O,O,O,B-PRO,I-PRO,I-PRO,O,O,O,...</td>\n      <td>31</td>\n    </tr>\n    <tr>\n      <th>756</th>\n      <td>柯 尼 卡 美 能 达 黑 白 多 功 能 复 合 机 主 机 双 面 器 双 面 自 动 ...</td>\n      <td>O,O,O,O,O,O,O,O,B-PRO,I-PRO,I-PRO,I-PRO,I-PRO,...</td>\n      <td>27</td>\n    </tr>\n    <tr>\n      <th>707</th>\n      <td>长 城 黑 白 激 光 复 印 机 适 配 国 产 平 台 自 动 双 面 打 印 、 复 印</td>\n      <td>B-BRA,I-BRA,O,O,O,O,B-PRO,I-PRO,I-PRO,O,O,O,O,...</td>\n      <td>24</td>\n    </tr>\n    <tr>\n      <th>699</th>\n      <td>联 想 激 光 多 功 能 一 体 机 四 合 一 黑 白 页 双 面 网 络 电 话</td>\n      <td>B-BRA,I-BRA,O,O,B-PRO,I-PRO,I-PRO,I-PRO,I-PRO,...</td>\n      <td>22</td>\n    </tr>\n    <tr>\n      <th>689</th>\n      <td>惠 普 喷 墨 一 体 机 幅 面 彩 色 页 分 钟 打 印 扫 描 复 印 传 真</td>\n      <td>B-BRA,I-BRA,B-PRO,I-PRO,I-PRO,I-PRO,I-PRO,O,O,...</td>\n      <td>22</td>\n    </tr>\n    <tr>\n      <th>705</th>\n      <td>奔 图 黑 白 激 光 打 印 机 适 配 国 产 平 台 自 动 双 面 打 印</td>\n      <td>B-BRA,I-BRA,O,O,O,O,B-PRO,I-PRO,I-PRO,O,O,O,O,...</td>\n      <td>21</td>\n    </tr>\n    <tr>\n      <th>706</th>\n      <td>奔 图 彩 色 激 光 打 印 机 适 配 国 产 平 台 自 动 双 面 打 印</td>\n      <td>B-BRA,I-BRA,O,O,O,O,B-PRO,I-PRO,I-PRO,O,O,O,O,...</td>\n      <td>21</td>\n    </tr>\n    <tr>\n      <th>754</th>\n      <td>柯 尼 卡 美 能 达 黑 白 复 印 机 配 双 面 器 双 面 自 动 输 稿</td>\n      <td>O,O,O,O,O,O,O,O,B-PRO,I-PRO,I-PRO,O,O,O,O,O,O,...</td>\n      <td>21</td>\n    </tr>\n    <tr>\n      <th>682</th>\n      <td>柯 尼 卡 美 能 达 双 面 自 动 进 稿 器 黑 白 多 功 能 复 合 机</td>\n      <td>O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,B-PRO,I-PRO,I-PR...</td>\n      <td>21</td>\n    </tr>\n    <tr>\n      <th>694</th>\n      <td>鸿 合 会 议 平 板 锐 系 列 交 互 电 子 白 板 教 学 一 体 机 英</td>\n      <td>B-BRA,I-BRA,B-PRO,I-PRO,I-PRO,I-PRO,O,O,O,O,O,...</td>\n      <td>21</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('data_process/data.csv', usecols=['sentence', 'word_labels'], encoding='ansi')\n",
    "data['word_labels'] = data['word_labels'].apply(lambda x: x.strip(\",\"))\n",
    "data['len_sen'] = data['sentence'].apply(lambda x: len(x.split(\" \")))\n",
    "data.sort_values(by='len_sen', ascending=False).head(10)  # 确定最大长度为21"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [
    {
     "data": {
      "text/plain": "{'O': 0, 'B-BRA': 1, 'I-BRA': 2, 'B-PRO': 3, 'I-PRO': 4}"
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tags = ['O', 'B-BRA', 'I-BRA', 'B-PRO', 'I-PRO']  # 定义标签种类\n",
    "labels_to_ids = {k: v for v, k in enumerate(tags)}  # 将标签转化为id\n",
    "ids_to_labels = {v: k for v, k in enumerate(tags)}\n",
    "labels_to_ids"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-27T03:14:20.410134500Z",
     "start_time": "2023-08-27T03:14:20.342526Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "f5EHpuB78pIa"
   },
   "source": [
    "#### **构建DataLoader**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lgNSM8Xz79Mg",
    "tags": [],
    "ExecuteTime": {
     "end_time": "2023-08-27T03:14:23.017192100Z",
     "start_time": "2023-08-27T03:14:22.971344800Z"
    }
   },
   "outputs": [],
   "source": [
    "MAX_LEN = 21  # 在数据处理阶段得到的结论\n",
    "TRAIN_BATCH_SIZE = 4\n",
    "VALID_BATCH_SIZE = 2\n",
    "EPOCHS = 4  # 训练轮次\n",
    "LEARNING_RATE = 1e-05  # 设置学习率\n",
    "MAX_GRAD_NORM = 5  # 用于梯度剪切\n",
    "MODEL_NAME=r'D:\\Asoftware\\Pycharm_pro\\venv\\NLP\\model\\BERT'  # 填写从hugging face上下载的模型存放位置\n",
    "tokenizer = BertTokenizer.from_pretrained(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [],
   "source": [
    "# 模型保存参数\n",
    "save_dir = \"./model\"\n",
    "if not os.path.exists(save_dir):\n",
    "    os.makedirs(save_dir)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-27T03:15:01.290954500Z",
     "start_time": "2023-08-27T03:15:01.269555800Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "D013FCEDAE3D46098F9C5502AF922AFA",
    "ExecuteTime": {
     "end_time": "2023-08-27T03:15:03.964747900Z",
     "start_time": "2023-08-27T03:15:03.884725800Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "sentence                               卡 萨 帝 波 轮 洗 衣 机\nword_labels    B-BRA,I-BRA,I-BRA,O,O,B-PRO,I-PRO,I-PRO\nlen_sen                                              8\nName: 0, dtype: object"
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# example\n",
    "data.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aJty_Abw8_xK",
    "ExecuteTime": {
     "end_time": "2023-08-27T03:15:13.613002Z",
     "start_time": "2023-08-27T03:15:13.591415600Z"
    }
   },
   "outputs": [],
   "source": [
    "class dataset(Dataset):\n",
    "    def __init__(self, dataframe, tokenizer, max_len):\n",
    "        self.len = len(dataframe)\n",
    "        self.data = dataframe\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        \n",
    "    def __getitem__(self, index):  # 将相应位置上的词/句转化为tensor向量\n",
    "        # 步骤 1: 对每个句子分词\n",
    "        sentence = self.data.sentence[index]  \n",
    "        word_labels = self.data.word_labels[index]\n",
    "        tokenized_sentence, labels = sentence.split(\" \"), word_labels.split(\",\")  # 简单用split分词即可\n",
    "        \n",
    "        # 步骤 2: 添加特殊token并添加对应的标签\n",
    "        tokenized_sentence = [\"[CLS]\"] + tokenized_sentence + [\"[SEP]\"] # add special tokens\n",
    "        labels.insert(0, \"O\") # 给[CLS] token添加O标签\n",
    "        labels.insert(-1, \"O\") # 给[SEP] token添加O标签\n",
    "\n",
    "        # 步骤 3: 截断/填充\n",
    "        maxlen = self.max_len\n",
    "\n",
    "        if len(tokenized_sentence) > maxlen:\n",
    "          # 截断\n",
    "          tokenized_sentence = tokenized_sentence[:maxlen]\n",
    "          labels = labels[:maxlen]\n",
    "        else:\n",
    "          # 填充\n",
    "          tokenized_sentence = tokenized_sentence + ['[PAD]'for _ in range(maxlen - len(tokenized_sentence))]\n",
    "          labels = labels + [\"O\" for _ in range(maxlen - len(labels))]\n",
    "\n",
    "        # 步骤 4: 构建attention mask\n",
    "        attn_mask = [1 if tok != '[PAD]' else 0 for tok in tokenized_sentence]\n",
    "        \n",
    "        # 步骤 5: 将分词结果转为词表的id表示\n",
    "        ids = self.tokenizer.convert_tokens_to_ids(tokenized_sentence)\n",
    "        label_ids = [labels_to_ids[label] for label in labels]\n",
    "\n",
    "        return {\n",
    "              'ids': torch.tensor(ids, dtype=torch.long),\n",
    "              'mask': torch.tensor(attn_mask, dtype=torch.long),\n",
    "              'targets': torch.tensor(label_ids, dtype=torch.long)\n",
    "        } \n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "id": "0722E6487C9B4D699D6A88B9CE5E1ED3",
    "ExecuteTime": {
     "end_time": "2023-08-27T03:15:26.457601700Z",
     "start_time": "2023-08-27T03:15:26.409184400Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FULL Dataset: (1032, 3)\n",
      "TRAIN Dataset: (826, 3)\n",
      "TEST Dataset: (206, 3)\n"
     ]
    }
   ],
   "source": [
    "# 数据集划分\n",
    "train_size = 0.8\n",
    "train_dataset = data.sample(frac=train_size,random_state=200)\n",
    "test_dataset = data.drop(train_dataset.index).reset_index(drop=True)\n",
    "train_dataset = train_dataset.reset_index(drop=True)\n",
    "\n",
    "print(\"FULL Dataset: {}\".format(data.shape))\n",
    "print(\"TRAIN Dataset: {}\".format(train_dataset.shape))\n",
    "print(\"TEST Dataset: {}\".format(test_dataset.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "outputs": [],
   "source": [
    "training_set = dataset(train_dataset, tokenizer, MAX_LEN)\n",
    "testing_set = dataset(test_dataset, tokenizer, MAX_LEN)\n",
    "\n",
    "# 创建Dataloader\n",
    "train_params = {'batch_size': TRAIN_BATCH_SIZE,\n",
    "                'shuffle': True,\n",
    "                'num_workers': 0\n",
    "                }\n",
    "\n",
    "test_params = {'batch_size': VALID_BATCH_SIZE,\n",
    "                'shuffle': True,\n",
    "                'num_workers': 0\n",
    "                }\n",
    "\n",
    "training_loader = DataLoader(training_set, **train_params)\n",
    "testing_loader = DataLoader(testing_set, **test_params)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-27T03:15:33.702823700Z",
     "start_time": "2023-08-27T03:15:33.659691800Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "phmPylgAm8Xy",
    "outputId": "27a7cee8-c920-4602-c539-b117ce6236ec",
    "ExecuteTime": {
     "end_time": "2023-08-27T03:15:35.226441600Z",
     "start_time": "2023-08-27T03:15:35.189801900Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "{'ids': tensor([ 101, 5401, 4638, 5892, 4171,  671,  860, 3322,  102,    0,    0,    0,\n            0,    0,    0,    0,    0,    0,    0,    0,    0]),\n 'mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n 'targets': tensor([0, 1, 2, 3, 4, 4, 4, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])}"
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# example\n",
    "training_set[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "DWgnNJrYW2GP",
    "outputId": "2b5b5b04-b93c-4354-b104-10b96c7223ff",
    "scrolled": true,
    "tags": [],
    "ExecuteTime": {
     "end_time": "2023-08-27T03:15:36.504944Z",
     "start_time": "2023-08-27T03:15:36.457696700Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS]       0   O\n",
      "美           1   B-BRA\n",
      "的           2   I-BRA\n",
      "蒸           3   B-PRO\n",
      "烤           4   I-PRO\n",
      "一           4   I-PRO\n",
      "体           4   I-PRO\n",
      "机           0   O\n",
      "[SEP]       4   I-PRO\n",
      "[PAD]       0   O\n",
      "[PAD]       0   O\n",
      "[PAD]       0   O\n",
      "[PAD]       0   O\n",
      "[PAD]       0   O\n",
      "[PAD]       0   O\n",
      "[PAD]       0   O\n",
      "[PAD]       0   O\n",
      "[PAD]       0   O\n",
      "[PAD]       0   O\n",
      "[PAD]       0   O\n",
      "[PAD]       0   O\n"
     ]
    }
   ],
   "source": [
    "# example\n",
    "for token, label in zip(tokenizer.convert_ids_to_tokens(training_set[0][\"ids\"]), training_set[0][\"targets\"]):\n",
    "  print('{0:10}  {1}   {2}'.format(token, label,ids_to_labels[label.numpy().tolist()]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "73OzU7oXRxR8"
   },
   "source": [
    "#### **载入预训练模型**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "cB9MR3KcWXUs",
    "outputId": "c0d93a4f-cd40-4f6b-edab-6dd51f2ee33b",
    "tags": [],
    "ExecuteTime": {
     "end_time": "2023-08-27T03:15:40.636961200Z",
     "start_time": "2023-08-27T03:15:38.698536800Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at D:\\Asoftware\\Pycharm_pro\\venv\\NLP\\model\\BERT and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": "BertForTokenClassification(\n  (bert): BertModel(\n    (embeddings): BertEmbeddings(\n      (word_embeddings): Embedding(21128, 768, padding_idx=0)\n      (position_embeddings): Embedding(512, 768)\n      (token_type_embeddings): Embedding(2, 768)\n      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (encoder): BertEncoder(\n      (layer): ModuleList(\n        (0): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (1): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (2): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (3): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (4): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (5): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (6): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (7): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (8): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (9): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (10): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (11): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n  )\n  (dropout): Dropout(p=0.1, inplace=False)\n  (classifier): Linear(in_features=768, out_features=5, bias=True)\n)"
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = BertForTokenClassification.from_pretrained(MODEL_NAME, num_labels=len(labels_to_ids))  # num_labels在此处为5\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Pp7Yl4JyWhDj"
   },
   "source": [
    "#### **模型训练**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kznSQfGIWdU4",
    "ExecuteTime": {
     "end_time": "2023-08-27T03:15:43.284423Z",
     "start_time": "2023-08-27T03:15:43.250446900Z"
    }
   },
   "outputs": [],
   "source": [
    "# 设置优化器Adam\n",
    "optimizer = torch.optim.Adam(params=model.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GLFivpkwW1HY",
    "ExecuteTime": {
     "end_time": "2023-08-27T03:17:30.602082200Z",
     "start_time": "2023-08-27T03:15:44.263092Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch: 1\n",
      "Training loss per 50 training steps: 1.692384958267212\n",
      "Training loss per 50 training steps: 0.6092148031674179\n",
      "Training loss per 50 training steps: 0.4033998487756984\n",
      "Training loss per 50 training steps: 0.3096090164282267\n",
      "Training loss per 50 training steps: 0.25658349212796533\n",
      "Training loss epoch: 0.2507974176267207\n",
      "Training accuracy epoch: 0.8417194424272623\n",
      "Training epoch: 2\n",
      "Training loss per 50 training steps: 0.08762291073799133\n",
      "Training loss per 50 training steps: 0.07374300638798113\n",
      "Training loss per 50 training steps: 0.07248822658202879\n",
      "Training loss per 50 training steps: 0.06767170027897551\n",
      "Training loss per 50 training steps: 0.06692384234606404\n",
      "Training loss epoch: 0.06578759119310529\n",
      "Training accuracy epoch: 0.9594467327489915\n",
      "Training epoch: 3\n",
      "Training loss per 50 training steps: 0.08238206803798676\n",
      "Training loss per 50 training steps: 0.03588311563191168\n",
      "Training loss per 50 training steps: 0.03649087980767657\n",
      "Training loss per 50 training steps: 0.03338336657319489\n",
      "Training loss per 50 training steps: 0.031937870355693634\n",
      "Training loss epoch: 0.0322488531321827\n",
      "Training accuracy epoch: 0.9790184794368789\n",
      "Training epoch: 4\n",
      "Training loss per 50 training steps: 0.007364734075963497\n",
      "Training loss per 50 training steps: 0.024251448538373497\n",
      "Training loss per 50 training steps: 0.017486346819862206\n",
      "Training loss per 50 training steps: 0.020432574921966044\n",
      "Training loss per 50 training steps: 0.019837553241053848\n",
      "Training loss epoch: 0.01950925236493619\n",
      "Training accuracy epoch: 0.9896361026156135\n"
     ]
    }
   ],
   "source": [
    "# 训练函数\n",
    "def train():\n",
    "    tr_loss, tr_accuracy = 0, 0\n",
    "    nb_tr_examples, nb_tr_steps = 0, 0\n",
    "    tr_preds, tr_labels = [], []\n",
    "    # 将model设置为train模式\n",
    "    model.train()\n",
    "    \n",
    "    for idx, batch in enumerate(training_loader):\n",
    "        \n",
    "        ids = batch['ids'].to(device, dtype = torch.long)\n",
    "        mask = batch['mask'].to(device, dtype = torch.long)\n",
    "        targets = batch['targets'].to(device, dtype = torch.long)\n",
    "\n",
    "        outputs = model(input_ids=ids, attention_mask=mask, labels=targets)\n",
    "        loss, tr_logits = outputs[0], outputs[1]\n",
    "        tr_loss += loss.item()\n",
    "\n",
    "        nb_tr_steps += 1\n",
    "        nb_tr_examples += targets.size(0)\n",
    "        \n",
    "        if idx % 50==0:\n",
    "            loss_step = tr_loss/nb_tr_steps\n",
    "            print(f\"Training loss per 50 training steps: {loss_step}\")\n",
    "           \n",
    "        # 计算准确率\n",
    "        flattened_targets = targets.view(-1) # 真实标签 大小 (batch_size * seq_len,)\n",
    "        active_logits = tr_logits.view(-1, model.num_labels) # 模型输出shape (batch_size * seq_len, num_labels)\n",
    "        flattened_predictions = torch.argmax(active_logits, axis=1) # 取出每个token对应概率最大的标签索引 shape (batch_size * seq_len,)\n",
    "        # MASK：PAD\n",
    "        active_accuracy = mask.view(-1) == 1 # shape (batch_size * seq_len,)\n",
    "        targets = torch.masked_select(flattened_targets, active_accuracy)\n",
    "        predictions = torch.masked_select(flattened_predictions, active_accuracy)\n",
    "        \n",
    "        tr_preds.extend(predictions)\n",
    "        tr_labels.extend(targets)\n",
    "        \n",
    "        tmp_tr_accuracy = accuracy_score(targets.cpu().numpy(), predictions.cpu().numpy())\n",
    "        tr_accuracy += tmp_tr_accuracy\n",
    "    \n",
    "        # 梯度剪切\n",
    "        torch.nn.utils.clip_grad_norm_(\n",
    "            parameters=model.parameters(), max_norm=MAX_GRAD_NORM\n",
    "        )\n",
    "        \n",
    "        # loss反向求导\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    epoch_loss = tr_loss / nb_tr_steps\n",
    "    tr_accuracy = tr_accuracy / nb_tr_steps\n",
    "    print(f\"Training loss epoch: {epoch_loss}\")\n",
    "    print(f\"Training accuracy epoch: {tr_accuracy}\")\n",
    "\n",
    "# 开始训练\n",
    "for epoch in range(EPOCHS):\n",
    "    print(f\"Training epoch: {epoch + 1}\")\n",
    "    train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "r4jcSOJr680a"
   },
   "source": [
    "#### **模型评估**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RIVVfFHi7Aw7",
    "ExecuteTime": {
     "end_time": "2023-08-27T03:17:40.520075300Z",
     "start_time": "2023-08-27T03:17:37.795471600Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss per 100 evaluation steps: 0.0574503056704998\n",
      "Validation loss per 100 evaluation steps: 0.055126491601528174\n",
      "Validation Loss: 0.05764250751941926\n",
      "Validation Accuracy: 0.9728624175047498\n"
     ]
    }
   ],
   "source": [
    "def valid(model, testing_loader):\n",
    "    # put model in evaluation mode\n",
    "    model.eval()\n",
    "    \n",
    "    eval_loss, eval_accuracy = 0, 0\n",
    "    nb_eval_examples, nb_eval_steps = 0, 0\n",
    "    eval_preds, eval_labels = [], []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for idx, batch in enumerate(testing_loader):\n",
    "            \n",
    "            ids = batch['ids'].to(device, dtype = torch.long)\n",
    "            mask = batch['mask'].to(device, dtype = torch.long)\n",
    "            targets = batch['targets'].to(device, dtype = torch.long)\n",
    "            \n",
    "            # loss, eval_logits = model(input_ids=ids, attention_mask=mask, labels=targets)\n",
    "            outputs = model(input_ids=ids, attention_mask=mask, labels=targets)\n",
    "            loss, eval_logits = outputs[0],outputs[1]\n",
    "            eval_loss += loss.item()\n",
    "\n",
    "            nb_eval_steps += 1\n",
    "            nb_eval_examples += targets.size(0)\n",
    "        \n",
    "            if idx % 100==0:\n",
    "                loss_step = eval_loss/nb_eval_steps\n",
    "                print(f\"Validation loss per 100 evaluation steps: {loss_step}\")\n",
    "              \n",
    "            # 计算准确率\n",
    "            flattened_targets = targets.view(-1) # 大小 (batch_size * seq_len,)\n",
    "            active_logits = eval_logits.view(-1, model.num_labels) # 大小 (batch_size * seq_len, num_labels)\n",
    "            flattened_predictions = torch.argmax(active_logits, axis=1) # 大小 (batch_size * seq_len,)\n",
    "            active_accuracy = mask.view(-1) == 1 # 大小 (batch_size * seq_len,)\n",
    "            targets = torch.masked_select(flattened_targets, active_accuracy)\n",
    "            predictions = torch.masked_select(flattened_predictions, active_accuracy)\n",
    "            \n",
    "            eval_labels.extend(targets)\n",
    "            eval_preds.extend(predictions)\n",
    "            \n",
    "            tmp_eval_accuracy = accuracy_score(targets.cpu().numpy(), predictions.cpu().numpy())\n",
    "            eval_accuracy += tmp_eval_accuracy\n",
    "\n",
    "    labels = [ids_to_labels[id.item()] for id in eval_labels]\n",
    "    predictions = [ids_to_labels[id.item()] for id in eval_preds]\n",
    "\n",
    "    eval_loss = eval_loss / nb_eval_steps\n",
    "    eval_accuracy = eval_accuracy / nb_eval_steps\n",
    "    print(f\"Validation Loss: {eval_loss}\")\n",
    "    print(f\"Validation Accuracy: {eval_accuracy}\")\n",
    "\n",
    "    return labels, predictions\n",
    "\n",
    "# 开始评估\n",
    "labels, predictions = valid(model, testing_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sqDklprSqB5d"
   },
   "source": [
    "#### **模型保存**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sDZtSsKKntuI",
    "outputId": "79b2c502-fbd3-46b3-eb54-f6d53d9563d1",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # 保存tokenizer\n",
    "# tokenizer.save_vocabulary(save_dir)\n",
    "# # 保存权重和配置文件\n",
    "# model.save_pretrained(save_dir)\n",
    "# print('Finished')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "language": "python",
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "name": "python",
   "mimetype": "text/x-python",
   "nbconvert_exporter": "python",
   "file_extension": ".py",
   "version": "3.5.2",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
